what is composite indexing? what are different types of indexing?
how mongodb handles replica set ?
what is sharding and partitioning ?
what is normalizations ?



what is ACID properties in dbms --------------
what is diff of mongo and postgres --------------

what are the stategies used for data migration and optimzing queries? - Adilisium consulting




-------------what is composite indexing
 https://stackoverflow.com/questions/795031/how-do-composite-indexes-work

 Composite indexes work just like regular indexes, except they have multi-values keys.
 If you define an index on the fields (a,b,c) , the records are sorted first on a, then b, then c.

 Example:
 | A | B | C |
 -------------
 | 1 | 2 | 3 |
 | 1 | 4 | 2 |
 | 1 | 4 | 4 |
 | 2 | 3 | 5 |
 | 2 | 4 | 4 |
 | 2 | 4 | 5 |

-------- how mongodb handles replica set 

------- what is partitioning and sharding ---------
Database sharding and partitioning are techniques used to manage and scale large datasets, but they differ in their approach to data distribution.

---------- partitioning -------
Partitioning involves dividing a large database table into smaller, more manageable units called partitions within a single database instance. 
This can be done horizontally (splitting rows based on a criterion) or vertically (splitting columns). 
The primary goals of partitioning are to improve query performance by reducing the amount of data to be scanned, enhance manageability (e.g., faster backups, index rebuilding), and facilitate data lifecycle management.

---------- what is sharding --------------
1st def:

 Sharding is a database architecture pattern related to horizontal partitioning â€” 
 the practice of ****separating one table's rows into multiple different tables****, known as partitions, that all have the same schema. 
 Each partition is referred to as a shard. Each shard is held on a separate database server instance, to spread load.

2nd def:

Sharding is a form of horizontal partitioning where data is distributed across multiple database instances or servers, known as shards. 
Each shard contains a subset of the total data and operates as an independent database.
Sharding is primarily used for horizontal scalability, allowing a system to handle higher transaction volumes and larger datasets by distributing the load across multiple machines. 
This helps to overcome the limitations of a single server, such as CPU, memory, and I/O capacity.

Key Differences:
Data Distribution: Partitioning keeps all data within a single database instance, albeit in separate logical segments. Sharding distributes data across multiple, independent database instances.
Scalability: Partitioning improves performance and manageability within a single server. Sharding enables horizontal scalability by adding more servers to the system.
Complexity: Sharding generally introduces more complexity in terms of data routing, distributed transactions, and maintaining data consistency across multiple servers. Partitioning within a single instance is typically less complex to manage.

In essence:
Partitioning helps to optimize performance and manageability within a single database server.
Sharding helps to scale out a database system across multiple servers to handle massive data volumes and high traffic.

----- what is CAP theorem --------------

 The CAP theorem, also known as Brewer's theorem, states that in a distributed data store, you can only achieve two out of the following three guarantees simultaneously:
 Consistency: Every read receives the most recent write or an error.
 Availability: Every request receives a (non-error) response, without the guarantee that it contains the most recent write.
 Partition Tolerance: The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes. 
 In practice, distributed systems must choose between consistency and availability when a network partition occurs.

--------- what is ACID properties in dbms --------------
 ACID is an acronym that stands for 
 Atomicity, Consistency, Isolation, and Durability. These are a set of properties that guarantee reliable processing of database transactions.  

 Atomicity: Ensures that all operations within a transaction are completed successfully. If any operation fails, the entire transaction is rolled back, leaving the database unchanged.    
 Consistency: Ensures that a transaction brings the database from one valid state to another, maintaining all predefined rules, such as integrity constraints.
 Isolation: Ensures that concurrent transactions do not interfere with each other. The intermediate state of a transaction is invisible to other transactions until it is committed.
 Durability: Ensures that once a transaction has been committed, it will remain so, even in the event of a system failure. This is typically achieved through the use of transaction logs and database backups.
 
  These properties are crucial for maintaining the integrity and reliability of databases, especially in systems where multiple transactions may occur simultaneously.

 ----------3. what is diff of mongo and postgres --------------

 MongoDB is a NoSQL document database that stores data in flexible, JSON-like documents and excels at ******* handling unstructured data and horizontal scalability********,
 while PostgreSQL is a powerful open-source object-relational database that uses tables with rows and columns, is known for its ACID compliance, 
 strong support for complex SQL queries, and reliability for structured data. 

 The core difference lies in their data models: 
 MongoDB uses a dynamic schema for flexibility, 
 whereas PostgreSQL requires a pre-defined, strict schema for data consistency.  



------------What is the difference between a primary key, foreign key, candidate key, and super key?

Primary key uniquely identifies a record, foreign key links tables, candidate key can be primary key, super key is a set of attributes.

Primary key: Unique identifier for a record in a table

Foreign key: Links tables together by referencing the primary key of another table

Candidate key: A set of attributes that can be used as a primary key

Super key: A set of attributes that can uniquely identify a record

-------------- 7 stagtegies for scaling databases (bytebytego) ----------

1. indexing
2. Materialized views
3. Denormalization
4. Vertical scaling
5. Database caching 
6. Replication
7. Sharding 



1. indexing ( b tree indexing)

Indexes can significantly  reduce query execution time Without proper indexing, even a  simple search query could turn  into a full table scan, which  is extremely time-consuming.


2. Materialized views

pre-computed snapshots  of data that are stored for faster access.
useful for  complex queries that would be too slow to compute on the fly every time. - tablleau 
This allows the reports to be  generated quickly and efficiently.


3. Denormalization

Denormalization involves storing redundant data  to reduce the complexity of database  queries and speed up data retrieval.
Facebook denormalizes data to store user  posts and information in the same table.
This approach minimizes the need  for complex joins between tables,  
speeding up retrieval when displaying user feeds.

Storing redundant data means that updates must be carefully managed to maintain  consistency across the database.

This added complexity in maintaining consistent data can lead to potential  issues if handled incorrectly.

4. Vertical scaling

Adding more resources such as cpu, ram, or storage to you existing db server to handle increased load



5. Database caching 

Caching involves storing frequently accessed  data in a faster storage layer to reduce the  load on your database and speed up response times.


6. Replication

Replication involves creating copies of  your primary database on different servers  to improve availability, distribute  the load, and enhance fault tolerance.
Replication can be configured in several ways,  such as synchronous or asynchronous replication. In synchronous replication, data is copied  to the replica servers simultaneously as  
it's written to the primary server,  ensuring immediate consistency.


7. Sharding 

Sharding is a database architecture  pattern that involves splitting a  large database into smaller, more  manageable pieces, called shards.
Each shard is a separate database  that contains a subset of the data.

Sharding is particularly effective  for scaling databases horizontally.
Instead of upgrading a single server's hardware,  you can add more servers to distribute the load.
Each server handles a portion of the data,  
which significantly enhances  both read and write performance.
However, sharding introduces complexity  in database design and management.


------------ why orm's ------
Key reasons for using ORMs

Increased development speed:  
ORMs automate common database tasks, eliminating the need to write and maintain repetitive SQL queries for simple create, read, update, and delete (CRUD) operations.

Reduced errors:     
By generating SQL queries automatically, ORMs decrease the risk of typos and other human errors common in writing complex SQL commands.

Improved code maintainability: 
ORMs allow developers to work with a familiar object-oriented paradigm. This makes the code more readable and easier for future developers (or even the original developer) to understand, modify, and extend.

Abstraction and portability: 
ORMs provide a layer of abstraction between your application and the database. This means the application's core logic doesn't need to be rewritten if you switch to a different type of database, as the ORM handles the necessary translations.

Enhanced security: 
ORMs automatically handle data sanitization, which helps prevent SQL injection attacks.

Automated schema management: 
Some ORMs can automatically generate database schemas, which further simplifies the management of database structures. 

-------------- what is  Kafka/RabbitMQ/SQS -----------




-------------how do we perform joins in mongodb
In MongoDB, which is a NoSQL, document-oriented database, the concept of "joins" as understood in relational databases is achieved using the $lookup operator within the aggregation pipeline. This operator performs a left outer join between two collections in the same database.
Here's how you perform joins in MongoDB using $lookup:
Initiate an Aggregation Pipeline: You begin by calling the aggregate() method on the collection you want to join from (the "left" collection).

  db.collection1.aggregate([
        // Aggregation stages go here
    ]);

Add the $lookup Stage: Inside the aggregation pipeline, you include the $lookup stage. This stage takes an object with the following parameters:
from: The name of the collection you want to join with (the "right" collection).
localField: The field from the input documents (from collection1) that will be used for the join.
foreignField: The field from the documents of the from collection (collection2) that will be used for the join.
as: The name of the new array field to add to the input documents. This array will contain the matching documents from the from collection.

    db.collection1.aggregate([
        {
            $lookup: {
                from: "collection2",       // The collection to join with
                localField: "fieldInCollection1", // Field from collection1
                foreignField: "fieldInCollection2", // Field from collection2
                as: "joinedData"          // Name of the new array field
            }
        }
    ]);

Process the Joined Data (Optional): After the $lookup stage, you can add other aggregation stages like $unwind, $match, $project, or $group to further process or shape the data in the joinedData array.
For example, to get only documents where a match was found in collection2:

    db.collection1.aggregate([
        {
            $lookup: {
                from: "collection2",
                localField: "fieldInCollection1",
                foreignField: "fieldInCollection2",
                as: "joinedData"
            }
        },
        {
            $match: { "joinedData": { $ne: [] } } // Filter out documents with empty joinedData arrays
        }
    ]);

Important Considerations:
Left Outer Join: $lookup performs a left outer join. This means all documents from the "left" collection (collection1 in the example) are returned, and if there are matching documents in the "right" collection (collection2), they are included in the as array. If no match is found, the as array will be empty for that document.
Data Modeling: While $lookup enables joins, a key principle of MongoDB is to store related data together (embedding or referencing) to minimize the need for joins and improve query performance. Consider your data model carefully to determine if joins are truly necessary or if a different modeling approach would be more efficient.
Sharded Collections: Starting in MongoDB 5.1, $lookup works across sharded collections.

------------- how to we do indexing in mongodb
Indexing in MongoDB is primarily achieved using the createIndex() method. This method allows you to define indexes on specific fields within a collection to improve query performance.

Creating a Single-Field Index:
To create an index on a single field, you use the following syntax:
db.<collection>.createIndex({ <field_name>: <order> })

<collection>: Replace this with the name of your collection.
<field_name>: Replace this with the name of the field you want to index. 
<order>: Specify 1 for ascending order or -1 for descending order.

To create an ascending index on the username field in a users collection:
db.users.createIndex({ username: 1 })

Creating a Compound Index:
You can also create indexes on multiple fields, known as compound indexes, to support queries that involve multiple criteria.
db.<collection>.createIndex({ <field1>: <order1>, <field2>: <order2>, ... })

To create a compound index on item (ascending) and stock (ascending) in a products collection:
db.products.createIndex({ item: 1, stock: 1 })

Optional Parameters:
The createIndex() method also accepts an optional options document where you can specify additional parameters, such as:
unique: Set to true to enforce uniqueness on the indexed field(s).
name: Provide a custom name for your index.
sparse: Create a sparse index that only indexes documents containing the indexed field.
expireAfterSeconds: For TTL (Time-to-Live) indexes, specify the time in seconds after which documents in the collection will expire and be removed.
Example with Options:

db.users.createIndex(
   { email: 1 },
   { unique: true, name: "email_unique_index" }
)

Viewing and Dropping Indexes:
To view existing indexes.
  db.<collection>.getIndexes()

  to drop an index.
  db.<collection>.dropIndex("<index_name>")

(You can find the index name using getIndexes(), or use the field specification if you didn't name it explicitly.)

Important Considerations:
Performance Trade-offs: While indexes improve read performance, they can add overhead to write operations (inserts, updates, deletes) as the index also needs to be updated.
Index Selection: Choose fields for indexing based on your common query patterns and fields used in sort operations.
Cardinality: Fields with high cardinality (many unique values) are generally better candidates for indexing.
Wildcard Indexes: For collections with varying document structures, wildcard indexes can be useful for indexing multiple fields dynamically.

------------- how to we do indexing in postgres 

To create an index on a table in PostgreSQL, use the CREATE INDEX statement.

CREATE INDEX index_name ON table_name (column_name);
CREATE INDEX idx_customer_email ON customers (email);

This creates an index named idx_customer_email on the email column of the customers table.


For multiple columns (composite index):

CREATE INDEX idx_order_customer_date ON orders (customer_id, order_date);

Unique index:
CREATE UNIQUE INDEX idx_unique_email ON customers (email);

Note:
Indexes improve query performance, especially for search, join, and sort operations.
Use EXPLAIN to analyze query plans and optimize indexing.

------------ what are diff types of join and what is inner join




// ============== iicl ==============================
// how do you handle mongodb connections failure to update db?

Handling MongoDB connection failures for updates involves using built-in driver mechanisms, implementing application-level retry logic with exponential backoff, and designing for resilience using replica sets and transactional operations. 
Driver-Level Handling (Recommended)
Modern MongoDB drivers handle many transient network errors and replica set elections automatically via retryable writes. This is the most effective and simplest approach for the developer. 
Enable Retryable Writes: For MongoDB version 4.2 and higher, this is enabled by default. For earlier versions, add retryWrites=true to your connection string. This allows the driver to automatically retry certain write operations once if they fail due to a temporary network issue or a primary node change.
Connection Pooling: Use the driver's built-in connection pooling. Configure options like maxPoolSize, connectTimeoutMS, and socketTimeoutMS to manage connection limits and timeouts effectively, preventing the application from hanging indefinitely on a bad connection. 
Application-Level Handling
For persistent errors or specific business logic, you must implement error handling in your application code. 
Use try-catch blocks or Promises: Wrap your database operations in error handling structures (e.g., try-catch in Node.js) to catch exceptions thrown by the driver.
Implement Custom Retry Logic: For errors that are transient (like network glitches or a temporary lack of a primary node during an election), you can implement custom retry logic. Use an exponential backoff strategy (increasing the delay between retries) to avoid overwhelming the database server.
Identify Transient Errors: Differentiate between transient errors (e.g., network timeout, MongoTimeoutError) and permanent errors (e.g., duplicate key error E11000, invalid command). Only retry transient faults.
Limit Retries: Set a maximum number of retries to prevent endless loops and potentially use a "circuit breaker" pattern to stop attempts if failures are persistent.
Log Errors: Always log errors in detail for debugging. Transient errors can be logged as warnings, while persistent failures should be logged as errors that may trigger alerts in your monitoring system.
Graceful Degradation/Fallback: If the update is not critical, consider writing the data to a local queue or a backup store (e.g., a message queue like Kafka) to be processed later when the connection is restored, rather than failing the user request. 
System & Infrastructure Best Practices
Use Replica Sets: Deploy MongoDB as a replica set (multiple servers). This provides high availability; if the primary node fails, the driver automatically finds the new primary, minimizing downtime during an update.
Monitor and Alert: Use monitoring tools (like MongoDB Atlas or Datadog) to track connection health, latency, and failure rates to proactively address underlying infrastructure issues.
Firewall & Network Configuration: Ensure firewall rules and network settings (e.g., IP access lists) are correctly configured to allow traffic between your application and all MongoDB instances. 


------------ postgres -------------
Handling PostgreSQL connection failures during a database update involves a combination of application-side retry logic, connection pooling, and robust system configuration. The specific approach depends on whether the failure is transient (temporary) or persistent. 
Application-Side Strategies
Most connection issues are transient and should be handled by the application logic. 
Implement Retry Logic with Exponential Backoff: The application should be designed to catch connection errors and retry the operation after a short wait period. Use an exponential backoff strategy (e.g., wait 5 seconds for the first retry, 10 for the second, up to a maximum like 60 seconds) to avoid overwhelming the database with connection requests during a potential outage. Set a maximum number of retries to prevent endless loops.
Use Connection Pooling: A connection pooler (like PgBouncer or a driver's built-in pool) sits between the application and the database. It manages a set of open connections efficiently, which helps in handling connection spikes and reducing the overhead of opening new connections for every request.
Configure the pool to validate connections before borrowing them to ensure they are still active (e.g., using a simple SELECT 1; validation query).
Set appropriate timeouts, such as statement_timeout and connectTimeout, to prevent queries or connection attempts from hanging indefinitely.
Handle Transactional Integrity: If a connection is lost during a transaction, the transaction will be automatically rolled back by PostgreSQL. The application's retry logic should attempt to re-run the entire transaction from the beginning. For critical, potentially duplicated updates, consider implementing idempotency using a unique client-generated ID to prevent duplicate entries if the original transaction actually committed but the acknowledgment was lost. 
System and Configuration Strategies
For persistent connection failures, the focus shifts to troubleshooting the infrastructure.
Verify Network and Firewall Settings: Ensure all firewalls (client-side, server-side, and network security groups) permit traffic on the PostgreSQL port (default 5432). Use tools like ping or telnet/nc to verify basic connectivity.
Check Server Status and Resources: Verify that the PostgreSQL server is running and has sufficient resources (CPU, memory, storage). A server running out of resources can drop connections.
Inspect postgresql.conf and pg_hba.conf:
listen_addresses should be set to '*' (or specific IPs) if connecting from a different host.
pg_hba.conf must have rules allowing connections from the client's IP range using the correct authentication method.
Monitor Logs: Review PostgreSQL server logs for specific error messages (e.g., authentication failures, resource issues) to pinpoint the exact cause of the persistent failure. 
By combining these application-level and configuration best practices, you can build a resilient system that handles connection failures gracefully and ensures data integrity during updates.


// ================ 
A MongoDB replica set is a group of mongod processes that hold the same data set, providing redundancy and high availability. It ensures a replica set can continue operating even if one member fails by using a primary node for all write operations and secondary nodes that replicate data from the primary. If the primary node goes down, a secondary node is elected to become the new primary, allowing for automatic failover. 
Key components and functions
Primary: The single mongod instance that receives all write operations from clients.
Secondaries: mongod instances that replicate the data from the primary to maintain an identical data set.
Automatic Failover: If the primary node fails, the remaining members of the replica set hold an election to choose a new primary.
High Availability: By having multiple copies of the data, the replica set can remain available for applications even during hardware failures or other interruptions.
Fault Tolerance: The redundancy provided by multiple copies of the data protects against the loss of a single server.
Read Scaling: Read operations can be distributed across the secondary nodes to distribute the load and improve performance.
Data Consistency: Secondary nodes are synchronized with the primary to ensure consistent data, though a small delay known as replication lag can occur. 
Minimum configuration
A minimum of three data-bearing members (one primary and two secondary members) is recommended for a production deployment. This ensures that a majority is always available to elect a new primary if needed. 


// ============ difference between left join and inner join ================

The main difference is that an INNER JOIN returns only the rows that have matching values in both tables, 
while a LEFT JOIN returns all rows from the left table and the matching rows from the right table. 


