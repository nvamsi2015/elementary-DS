what is composite indexing? what are different types of indexing?
how mongodb handles replica set ?
what is sharding ?
what is normalizations ?



what is ACID properties in dbms --------------
what is diff of mongo and postgres --------------

what are the stategies used for data migration and optimzing queries? - Adilisium consulting




-------------what is composite indexing
 https://stackoverflow.com/questions/795031/how-do-composite-indexes-work
 Composite indexes work just like regular indexes, except they have multi-values keys.
 If you define an index on the fields (a,b,c) , the records are sorted first on a, then b, then c.
 Example:
 | A | B | C |
 -------------
 | 1 | 2 | 3 |
 | 1 | 4 | 2 |
 | 1 | 4 | 4 |
 | 2 | 3 | 5 |
 | 2 | 4 | 4 |
 | 2 | 4 | 5 |
-------- how mongodb handles replica set 
---------- what is sharding --------------
 Sharding is a database architecture pattern related to horizontal partitioning â€” 
 the practice of separating one table's rows into multiple different tables, known as partitions, that all have the same schema. 
 Each partition is referred to as a shard. Each shard is held on a separate database server instance, to spread load.
----- what is CAP theorem --------------
 The CAP theorem, also known as Brewer's theorem, states that in a distributed data store, you can only achieve two out of the following three guarantees simultaneously:
 Consistency: Every read receives the most recent write or an error.
 Availability: Every request receives a (non-error) response, without the guarantee that it contains the most recent write.
 Partition Tolerance: The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes. 
 In practice, distributed systems must choose between consistency and availability when a network partition occurs.
--------- what is ACID properties in dbms --------------
 ACID is an acronym that stands for 
 Atomicity, Consistency, Isolation, and Durability. These are a set of properties that guarantee reliable processing of database transactions.  
 Atomicity: Ensures that all operations within a transaction are completed successfully. If any operation fails, the entire transaction is rolled back, leaving the database unchanged.    
 Consistency: Ensures that a transaction brings the database from one valid state to another, maintaining all predefined rules, such as integrity constraints.
 Isolation: Ensures that concurrent transactions do not interfere with each other. The intermediate state of a transaction is invisible to other transactions until it is committed.
 Durability: Ensures that once a transaction has been committed, it will remain so, even in the event of a system failure. This is typically achieved through the use of transaction logs and
 database backups.# These properties are crucial for maintaining the integrity and reliability of databases, especially in systems where multiple transactions may occur simultaneously.
 ----------3. what is diff of mongo and postgres --------------
 MongoDB is a NoSQL document database that stores data in flexible, JSON-like documents and excels at handling unstructured data and horizontal scalability,
 while PostgreSQL is a powerful open-source object-relational database that uses tables with rows and columns, is known for its ACID compliance, 
 strong support for complex SQL queries, and reliability for structured data. 
 The core difference lies in their data models: 
 MongoDB uses a dynamic schema for flexibility, 
 whereas PostgreSQL requires a pre-defined, strict schema for data consistency.  



------------What is the difference between a primary key, foreign key, candidate key, and super key?
Primary key uniquely identifies a record, foreign key links tables, candidate key can be primary key, super key is a set of attributes.

Primary key: Unique identifier for a record in a table

Foreign key: Links tables together by referencing the primary key of another table

Candidate key: A set of attributes that can be used as a primary key

Super key: A set of attributes that can uniquely identify a record

-------------- 7 stagtegies for scaling databases (bytebytego) ----------
1. indexing
2. Materialized views
3. Denormalization
4. Vertical scaling
5. Database caching 
6. Replication
7. Sharding 



1. indexing ( b tree indexing)

Indexes can significantly  reduce query execution time Without proper indexing, even a  simple search query could turn  into a full table scan, which  is extremely time-consuming.


2. Materialized views

pre-computed snapshots  of data that are stored for faster access.
useful for  complex queries that would be too slow to compute on the fly every time. - tablleau 
This allows the reports to be  generated quickly and efficiently.


3. Denormalization

Denormalization involves storing redundant data  to reduce the complexity of database  queries and speed up data retrieval.
Facebook denormalizes data to store user  posts and information in the same table.
This approach minimizes the need  for complex joins between tables,  
speeding up retrieval when displaying user feeds.
Storing redundant data means that updates must  
3:37
be carefully managed to maintain  consistency across the database.
3:41
This added complexity in maintaining consistent  
3:43
data can lead to potential  issues if handled incorrectly.

4. Vertical scaling

Adding more resources such as cpu, ram, or storage to you existing db server to handle increased load



5. Database caching 

Caching involves storing frequently accessed  data in a faster storage layer to reduce the  load on your database and speed up response times.


6. Replication

Replication involves creating copies of  your primary database on different servers  to improve availability, distribute  the load, and enhance fault tolerance.
Replication can be configured in several ways,  such as synchronous or asynchronous replication. In synchronous replication, data is copied  to the replica servers simultaneously as  
it's written to the primary server,  ensuring immediate consistency.


7. Sharding 

Sharding is a database architecture  pattern that involves splitting a  large database into smaller, more  manageable pieces, called shards.
Each shard is a separate database  that contains a subset of the data.

Sharding is particularly effective  for scaling databases horizontally.
Instead of upgrading a single server's hardware,  you can add more servers to distribute the load.
Each server handles a portion of the data,  
which significantly enhances  both read and write performance.
However, sharding introduces complexity  in database design and management.


------------ why orm's ------
Key reasons for using ORMs

Increased development speed:  
ORMs automate common database tasks, eliminating the need to write and maintain repetitive SQL queries for simple create, read, update, and delete (CRUD) operations.

Reduced errors:     
By generating SQL queries automatically, ORMs decrease the risk of typos and other human errors common in writing complex SQL commands.

Improved code maintainability: 
ORMs allow developers to work with a familiar object-oriented paradigm. This makes the code more readable and easier for future developers (or even the original developer) to understand, modify, and extend.

Abstraction and portability: 
ORMs provide a layer of abstraction between your application and the database. This means the application's core logic doesn't need to be rewritten if you switch to a different type of database, as the ORM handles the necessary translations.

Enhanced security: 
ORMs automatically handle data sanitization, which helps prevent SQL injection attacks.

Automated schema management: 
Some ORMs can automatically generate database schemas, which further simplifies the management of database structures. 

-------------- what is  Kafka/RabbitMQ/SQS -----------










